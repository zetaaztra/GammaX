name: Weekly Data Pipeline & Cleanup

on:
  schedule:
    - cron: "30 20 * * 0"  # Sunday 2:00 AM IST (8:30 PM UTC Saturday)
  workflow_dispatch:

jobs:
  cleanup-old-data:
    name: Clean up old cache and data
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Clean old workflow runs
        uses: actions/github-script@v7
        with:
          script: |
            const days_to_keep = 30;
            const cutoff = new Date(Date.now() - days_to_keep * 24 * 60 * 60 * 1000);
            
            const runs = await github.rest.actions.listWorkflowRunsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              per_page: 100
            });
            
            for (const run of runs.data.workflow_runs) {
              if (new Date(run.created_at) < cutoff) {
                console.log(`Deleting run ${run.id} from ${run.created_at}`);
                await github.rest.actions.deleteWorkflowRun({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: run.id
                });
              }
            }
      
      - name: Clear GitHub Actions cache
        run: |
          gh extension install actions/gh-actions-cache
          
          echo "Fetching list of cache keys"
          cacheKeys=$(gh actions-cache list -R $REPO -B $BRANCH | cut -f 1)
          
          echo "Deleting caches..."
          for cacheKey in $cacheKeys
          do
            gh actions-cache delete $cacheKey -R $REPO -B $BRANCH --confirm
          done
          
          echo "Done clearing cache"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          REPO: ${{ github.repository }}
          BRANCH: ${{ github.ref }}

  full-pipeline:
    name: Generate all stock data
    needs: cleanup-old-data
    runs-on: ubuntu-latest
    timeout-minutes: 50
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Fetch tickers list
        run: python scripts/fetch_tickers.py
      
      - name: Generate all 503 stocks data
        run: |
          python scripts/tradyxa_pipeline.py \
            --mode batch_run \
            --tickers-file scripts/nifty500.txt \
            --max-workers 4 \
            --use-yf
      
      - name: Train regime classification model
        run: python scripts/train_regime_classifier.py
      
      - name: Train slippage forecasting models
        run: python scripts/train_slippage_quantile.py
      
      - name: Apply ML predictions to all stocks
        run: python scripts/apply_models.py
      
      - name: Fetch live spot prices
        run: python scripts/fetch_spot_prices.py
      
      - name: Commit and push changes
        run: |
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions@github.com"
          git add public/data/ticker/*.json
          git add public/data/raw/*.csv
          git add public/data/live/spot_prices.json
          git add models/
          git diff --quiet && git diff --staged --quiet || \
            git commit -m "ðŸ“Š Weekly data pipeline update [skip ci]"
          git fetch origin main
          git rebase origin/main || git rebase --abort
          git push origin main
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Summary
        run: |
          echo "âœ… Weekly pipeline completed successfully!"
          echo "ðŸ“Š Generated data for 503 stocks"
          echo "ðŸ¤– Trained and applied ML models"
          echo "ðŸ’° Fetched live spot prices"
          echo "ðŸ§¹ Cleaned up old caches"
